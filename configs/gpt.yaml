architecture:
  n_embd: 2048
  n_layers: 32
  bias: true
  dropout: 0.0
  n_head: 16
  rope: true
  block_size: 512
  vocab_size: 100288
  vram_calib_factor: 1.0
eval:
  evaluations: []
optimization:
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  lr: 0.0003
training:
  batch_size: 512
  per_device_batch_size: 8
  tokens: 1000000000
  warmup_pct: 0.01
  decay_pct: 0.1
  validation: true
  evaluate: true
  dataset:
  - name: fineweb
    rate: 1.0
    style: PMD
    suffix: ''
  validation_steps: 2048
logging:
  report_interval: 32
  checkpoint_interval: 16384
  validation_interval: 2048
  wandb: false
job: gpt/train/pretrain
request:
  chip: h200
  min_chips: 2
