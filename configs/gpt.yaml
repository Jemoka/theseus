# uv run theseus submit baseline_regular ./configs/gpt.yaml -n 2 --chip h200 --mem 128G -p scratchbubbles -g e0

architecture:
  n_embd: 2048
  n_layers: 32
  bias: true
  dropout: 0.0
  n_head: 16
  rope: true
  block_size: 512
  vocab_size: 100288
  vram_calib_factor: 1.0
eval:
  evaluations:
  - blimp
tokenizer:
  backend: tiktoken
  name: cl100k_base
  huggingface:
    use_fast: true
    use_remote_code: false
optimization:
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  lr: 0.0003
  warmup_pct: 0.005
  decay_pct: 0.01
training:
  batch_size: 512
  per_device_batch_size: 8
  tokens: 40000000000
  warmup_pct: 0.01
  decay_pct: 0.1
  validation: true
  evaluate: false
  dataset:
  - name: fineweb
    rate: 1.0
    style: PMD
    suffix: ''
  validation_steps: 2048
logging:
  report_interval: 32
  checkpoint_interval: 16384
  validation_interval: 2048
  wandb: true
request:
  n_shards: 1
job: gpt/train/pretrain
