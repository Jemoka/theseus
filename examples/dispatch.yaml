# Host priority order for solver (first match wins)
priority:
  - workstation       # prefer local workstation
  - devbox            # then devbox
  - hpc-login         # then HPC cluster
  - cloud             # cloud as last resort

# Map chip names to SLURM GRES types (cluster-specific)
gres_mapping:
  h100: h100_80gb           # your cluster's GRES name for H100
  a100-sxm4-80gb: a100_80g  # GRES name for A100 80GB
  a100-pcie-40gb: a100_40g  # GRES name for A100 40GB

clusters:
  local:
    root: /data/theseus
    work: /tmp/theseus
    log: /var/log/theseus  # optional, defaults to {work}/logs

  hpc:
    root: /mnt/juicefs/theseus
    work: /scratch/theseus
    log: /scratch/theseus/logs
    share: /scratch/theseus/.dispatch  # shared dir for scripts (visible to all nodes)
    mount: redis://:password@redis.example.com:6379/0  # JuiceFS redis connection
    cache_size: 100G  # JuiceFS --cache-size
    cache_dir: /scratch/juicefs-cache  # JuiceFS --cache-dir

hosts:
  # Plain SSH host - direct GPU access, no scheduler
  workstation:
    ssh: gpu-workstation        # alias from ~/.ssh/config
    cluster: local
    type: plain
    chips:
      h100: 4
    uv_groups: [cuda12]         # uv sync --group cuda12

  devbox:
    ssh: devbox
    cluster: local
    type: plain
    chips:
      a100-sxm4-80gb: 8
    uv_groups: [cuda12, dev]

  # SLURM cluster - jobs submitted via sbatch
  hpc-login:
    ssh: hpc
    cluster: hpc
    type: slurm
    partitions: [gpu, cpu]
    account: myproject
    qos: normal
    mem: 64G                     # default memory per job (default: 64G if not set)
    exclude: [node001, node002]  # nodes to exclude (--exclude)
    uv_groups: [cuda12]

  # SLURM cluster with explicit chip allocation limits
  hpc-limited:
    ssh: hpc
    cluster: hpc
    type: slurm
    partitions: [gpu]
    account: myproject
    chips:                       # optional: limit allocation to these chips
      h100: 8                    # only use up to 8 H100s from this host
      a100-sxm4-80gb: 16        # only use up to 16 A100-80GBs
    uv_groups: [cuda12]

  cloud:
    ssh: cloud
    cluster: hpc
    type: slurm
    partitions: [a100, h100]
    uv_groups: [cuda11]         # older CUDA on this cluster
